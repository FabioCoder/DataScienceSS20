{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands on HDF5\n",
    "This brief lab session gives an overview over the basic usage of the HDF5 file format for Python applications.\n",
    "\n",
    "Work through the examples and try to change some of the settings...\n",
    "\n",
    "### Sources:\n",
    "* HDF5 documentation https://portal.hdfgroup.org/display/HDF5/HDF5\n",
    "* h5py API: http://docs.h5py.org/en/stable/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running in Colab: False\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "print('running in Colab:',IN_COLAB)\n",
    "path='..'\n",
    "if IN_COLAB:\n",
    "  #in colab, we need to clone the data from the repo\n",
    "  !git clone https://github.com/keuperj/DataScienceSS20.git\n",
    "  path='DataScienceSS20'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py #this is the HDF5 lib "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#create some random data\n",
    "matrix1 = np.random.random(size = (1000,1000))\n",
    "matrix2 = np.random.random(size = (10000,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# write it to the same file - in two different arrays\n",
    "with h5py.File('hdf5_data.h5', 'w') as hdf: #note the write mode 'w'\n",
    "    hdf.create_dataset('dataset1', data=matrix1)\n",
    "    hdf.create_dataset('dataset2', data=matrix2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of datasets in this file: \n",
      " ['dataset1', 'dataset2']\n",
      "Shape of dataset1: \n",
      " (10000, 100)\n"
     ]
    }
   ],
   "source": [
    "#opening, listing and reading files\n",
    "with h5py.File('hdf5_data.h5','r') as hdf:\n",
    "    ls = list(hdf.keys())\n",
    "    print('List of datasets in this file: \\n', ls)\n",
    "    data = hdf.get('dataset2') #here data is still some hdf5 object\n",
    "    dataset1 = np.array(data) #need to convert it into numpy\n",
    "    print('Shape of dataset1: \\n', dataset1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.91971145, 0.36230974, 0.31624066, ..., 0.00283999, 0.08925693,\n",
       "        0.57769745],\n",
       "       [0.23826873, 0.33277627, 0.08914027, ..., 0.6613984 , 0.44588789,\n",
       "        0.72270579],\n",
       "       [0.08313031, 0.05226765, 0.13767883, ..., 0.47499835, 0.70893577,\n",
       "        0.39471245],\n",
       "       ...,\n",
       "       [0.97627921, 0.77523194, 0.44976618, ..., 0.91766727, 0.29534689,\n",
       "        0.66616802],\n",
       "       [0.82574779, 0.45119996, 0.86465028, ..., 0.96875913, 0.42592237,\n",
       "        0.3807802 ],\n",
       "       [0.98316821, 0.83198641, 0.06704393, ..., 0.73154225, 0.27841907,\n",
       "        0.81031698]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File('hdf5_data.h5', 'r')\n",
    "ls = list(f.keys())\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dataset1', 'dataset2']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Array Slicing\n",
    "HDF5 support fancy array slicing - so we do not read all data just to get a slice: http://docs.h5py.org/en/latest/high/dataset.html#fancy-indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.92142997, 0.57541022, 0.3885701 , ..., 0.998872  , 0.00903802,\n",
       "        0.63366094],\n",
       "       [0.23449488, 0.28482728, 0.97169593, ..., 0.14189487, 0.65587375,\n",
       "        0.85942152],\n",
       "       [0.55885495, 0.42931871, 0.05031121, ..., 0.92223714, 0.89190211,\n",
       "        0.15570669],\n",
       "       ...,\n",
       "       [0.33034277, 0.23511998, 0.69677524, ..., 0.666284  , 0.91152655,\n",
       "        0.12095762],\n",
       "       [0.21625754, 0.5701883 , 0.35523836, ..., 0.53997096, 0.57559244,\n",
       "        0.68611711],\n",
       "       [0.79302093, 0.86823233, 0.81046259, ..., 0.0422652 , 0.07660074,\n",
       "        0.41612614]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = h5py.File('hdf5_data.h5', 'r')\n",
    "f['dataset1'][100:120,:] # this notation mostly follows numpy notation -> try different slices!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Groups\n",
    "We can organize data in groups, just like in file systems where we have files (here datasets) in folders (here groups) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix1 = np.random.random(size = (1000,1000))\n",
    "matrix2 = np.random.random(size = (1000,1000))\n",
    "matrix3 = np.random.random(size = (1000,1000))\n",
    "matrix4 = np.random.random(size = (1000,1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('hdf5_groups.h5', 'w') as hdf:\n",
    "    G1 = hdf.create_group('Group1')\n",
    "    G1.create_dataset('dataset1', data = matrix1)\n",
    "    G1.create_dataset('dataset4', data = matrix4)\n",
    " \n",
    "    G21 = hdf.create_group('Group2/SubGroup1')\n",
    "    G21.create_dataset('dataset3', data = matrix3)\n",
    "    \n",
    "    G22 = hdf.create_group('Group2/SubGroup2')\n",
    "    G22.create_dataset('dataset2', data = matrix2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items in the base directory: [('Group1', <HDF5 group \"/Group1\" (2 members)>), ('Group2', <HDF5 group \"/Group2\" (2 members)>)]\n",
      "Items in Group2: [('SubGroup1', <HDF5 group \"/Group2/SubGroup1\" (1 members)>), ('SubGroup2', <HDF5 group \"/Group2/SubGroup2\" (1 members)>)]\n",
      "Items in Group21: [('dataset3', <HDF5 dataset \"dataset3\": shape (1000, 1000), type \"<f8\">)]\n",
      "(1000, 1000)\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('hdf5_groups.h5','r') as hdf:\n",
    "    base_items = list(hdf.items())\n",
    "    print('Items in the base directory:', base_items)\n",
    "    G2 = hdf.get('Group2')\n",
    "    G2_items = list(G2.items())\n",
    "    print('Items in Group2:', G2_items)\n",
    "    G21 = G2.get('/Group2/SubGroup1')\n",
    "    G21_items = list(G21.items())\n",
    "    print('Items in Group21:', G21_items)\n",
    "    dataset3 = np.array(G21.get('dataset3'))\n",
    "    print(dataset3.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is happening? Interpret the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compress Data\n",
    "HDF5 also support native data compression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix1 = np.random.random(size = (1000,1000))\n",
    "matrix2 = np.random.random(size = (1000,1000))\n",
    "matrix3 = np.random.random(size = (1000,1000))\n",
    "matrix4 = np.random.random(size = (1000,1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('hdf5_groups_compressed.h5', 'w') as hdf:\n",
    "    G1 = hdf.create_group('Group1')\n",
    "    G1.create_dataset('dataset1', data = matrix1, compression=\"gzip\", compression_opts=9)\n",
    "    G1.create_dataset('dataset4', data = matrix4, compression=\"gzip\", compression_opts=9)\n",
    " \n",
    "    G21 = hdf.create_group('Group2/SubGroup1')\n",
    "    G21.create_dataset('dataset3', data = matrix3, compression=\"gzip\", compression_opts=9)\n",
    "    \n",
    "    G22 = hdf.create_group('Group2/SubGroup2')\n",
    "    G22.create_dataset('dataset2', data = matrix2, compression=\"gzip\", compression_opts=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attributes\n",
    "We can add meta information in form of attributes of files, groups and datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix1 = np.random.random(size = (1000,1000))\n",
    "matrix2 = np.random.random(size = (10000,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create the HDF5 file\n",
    "hdf = h5py.File('test.h5', 'w')\n",
    "\n",
    "# Create the datasets\n",
    "dataset1 = hdf.create_dataset('dataset1', data=matrix1)\n",
    "dataset2 = hdf.create_dataset('dataset2', data=matrix2)\n",
    "\n",
    "# Set attributes\n",
    "dataset1.attrs['CLASS'] = 'DATA MATRIX'\n",
    "dataset1.attrs['VERSION'] = '1.1'\n",
    "\n",
    "hdf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Read the HDF5 file\n",
    "hdf = h5py.File('test.h5', 'r')\n",
    "ls = list(hdf.keys())\n",
    "print('List of datasets in this file: \\n', ls)\n",
    "data = hdf.get('dataset1')\n",
    "dataset1 = np.array(data)\n",
    "print('Shape of dataset1: \\n', dataset1.shape)\n",
    "#read the attributes\n",
    "k = list(data.attrs.keys())\n",
    "v = list(data.attrs.values())\n",
    "print(k[0])\n",
    "print(v[0])\n",
    "print(data.attrs[k[0]])\n",
    "\n",
    "hdf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDF5 and ***Pandas***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# creates (or opens in append mode) an hdf5 file\n",
    "hdf = pd.HDFStore('hdf5_pandas.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(path+'/DATA/FL_insurance_sample.csv')# put the dataset in the storage\n",
    "hdf.put('DF1', df1, format='table', data_columns=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "data = {\n",
    "         \"city\": [\"Tripoli\", \"Sydney\", \"Tripoli\", \"Rome\", \"Rome\", \"Tripoli\",\"Rome\", \"Sydney\", \"Sydney\"],\n",
    "         \"rank\": [\"1st\", \"2nd\", \"1st\", \"2nd\", \"1st\", \"2nd\",\"1st\", \"2nd\", \"1st\"], \n",
    "         \"score1\": [44, 48, 39, 41, 38, 44, 34, 54, 61],\n",
    "         \"score2\": [67, 63, 55, 70, 64, 77, 45, 66, 72]\n",
    "        }\n",
    "        \n",
    "df2 = pd.DataFrame(data, columns = ['city', 'rank','score1','score2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "hdf.put('DF2Key', df2,format='table', data_columns=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf.close() # close the hdf5 file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### now read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# open hdf5 file for reading\n",
    "hdf = pd.HDFStore('hdf5_pandas.h5',mode='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "hdf.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df1 = hdf.get('/DF1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "type(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "hdf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
